{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# PACKAGE\n",
    "# First load the worksheet dependencies. / Primero cargue las dependencias de la hoja de trabajo\n",
    "# Here is the activation function and its derivative. / Aquí está la función de activación y su derivada\n",
    "\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "\n",
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "# Esta función inicializa la red con su estructura, también reinicia cualquier entrenamiento ya realizado.\n",
    "\n",
    "def reset_network (n1 = 6, n2 = 7, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(2, n2) / 2\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(2, 1) / 2\n",
    "\n",
    "    \n",
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "# Esta función avanza cada activación a la siguiente capa. Devuelve todas las sumas ponderadas y activaciones.\n",
    "\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "    return a0, z1, a1, z2, a2, z3, a3\n",
    "\n",
    "# This is the cost function of a neural network with respect to a training set.\n",
    "# Ésta es la función de costo de una red neuronal con respecto a un conjunto de entrenamiento.\n",
    "\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION \n",
    "# FUNCIÓN GRADADA\n",
    "\n",
    "# Jacobian for the third layer weights. There is no need to edit this function.\n",
    "# Jacobian para los pesos de la tercera capa. No es necesario editar esta función.\n",
    "\n",
    "def J_W3 (x, y) :\n",
    "    \n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    # Primero obtenga todas las activaciones y sumas ponderadas en cada capa de la red.\n",
    "    \n",
    "    \n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Usaremos la variable J para almacenar partes de nuestro resultado a medida que avanzamos, actualizándolo en cada línea.\n",
    "    \n",
    "    \n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    # En primer lugar, calculamos dC / da3, usando las expresiones anteriores.\n",
    "    \n",
    "    J = 2 * (a3 - y)\n",
    "    \n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    # Luego multiplique el resultado que hemos calculado por la derivada de sigma, evaluada en z3.\n",
    "    \n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    \n",
    "    # Then we take the dot product (along the axis that holds the training examples) with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    \n",
    "    \n",
    "    # Luego tomamos el producto escalar (a lo largo del eje que contiene los ejemplos de entrenamiento) con la derivada\n",
    "    # parcial final, es decir, dz3 / dW3 = a2\n",
    "    # y divida por el número de ejemplos de entrenamiento, para obtener el promedio de todos los ejemplos de entrenamiento.\n",
    "    \n",
    "    \n",
    "    \n",
    "    J = J @ a2.T / x.size\n",
    "    \n",
    "    \n",
    "    # Finally return the result out of the function.\n",
    "    # Finalmente devuelve el resultado de la función.\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In this function, you will implement the jacobian for the bias.\n",
    "# As you will see from the partial derivatives, only the last partial derivative is different.\n",
    "# The first two partial derivatives are the same as previously.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "\n",
    "\n",
    "\n",
    "# En esta función, implementará el jacobiano para el sesgo.\n",
    "# Como verá en las derivadas parciales, solo la última derivada parcial es diferente.\n",
    "# Las dos primeras derivadas parciales son las mismas que antes.\n",
    "# === DEBE EDITAR ESTA FUNCIÓN ===\n",
    "\n",
    "\n",
    "\n",
    "def J_b3 (x, y) :\n",
    "    \n",
    "    # As last time, we'll first set up the activations.\n",
    "    # Como la última vez, primero configuraremos las activaciones.\n",
    "    \n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    \n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    # ===COPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS===\n",
    "    \n",
    "    # A continuación, debe implementar las dos primeras derivadas parciales del jacobiano.\n",
    "    # === COPIAR DOS LÍNEAS DE LA FUNCIÓN ANTERIOR PARA CONFIGURAR LOS DOS PRIMEROS TÉRMINOS JACOBIANOS ===\n",
    "    \n",
    "    \n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    \n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    # There is no need to edit this line.\n",
    "    \n",
    "    # Para la línea final, no necesitamos multiplicar por dz3 / db3, porque eso es multiplicar por 1.\n",
    "    # Sin embargo, todavía tenemos que resumir todos los ejemplos de entrenamiento.\n",
    "    # No es necesario editar esta línea.\n",
    "    \n",
    "    \n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRADED FUNCTION\n",
    "\n",
    "# Compare this function to J_W3 to see how it changes.\n",
    "# There is no need to edit this function.\n",
    "\n",
    "# Compare esta función con J_W3 para ver cómo cambia.\n",
    "# No es necesario editar esta función.\n",
    "\n",
    "\n",
    "\n",
    "def J_W2 (x, y) :\n",
    "    \n",
    "    #The first two lines are identical to in J_W3.\n",
    "    # Las dos primeras líneas son idénticas a las de J_W3.\n",
    "    \n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    J = 2 * (a3 - y)\n",
    "    \n",
    "\n",
    "    # the next two lines implement da3/da2, first σ' and then W3. \n",
    "    # las siguientes dos líneas implementan da3 / da2, primero σ 'y luego W3.\n",
    "    \n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    \n",
    "    \n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    # entonces las líneas finales son las mismas que en J_W3 pero con el número de capa bajado.\n",
    "    \n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    return J\n",
    "\n",
    "\n",
    "# As previously, fill in all the incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "\n",
    "# Como anteriormente, complete todas las líneas incompletas. (aquí ya están completas, el comentario es el enunciado)\n",
    "\n",
    "\n",
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J\n",
    "\n",
    "\n",
    "# GRADED FUNCTION\n",
    "\n",
    "# Fill in all incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===  (resuelto)\n",
    "\n",
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)   # ∂a2 / ∂z2\n",
    "    J = (J.T @ W2).T      # ∂z2 / ∂a1\n",
    "    J = J * d_sigma(z1)   # ∂a1 / ∂z1\n",
    "    J = J @ a0.T / x.size\n",
    "    return J\n",
    "\n",
    "# Fill in all incomplete lines.\n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===  (resuelto)\n",
    "\n",
    "\n",
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)   # ∂a2 / ∂z2\n",
    "    J = (J.T @ W2).T      # ∂z2 / ∂a1\n",
    "    J = J * d_sigma(z1)   # ∂a1 / ∂z1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
